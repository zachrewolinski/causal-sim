{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp import get_batch\n",
    "from torch.nn import ReLU, Sigmoid\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def truncate(pscores, thresh):\n",
    "#     return np.clip(pscores, thresh, 1 - thresh)\n",
    "\n",
    "# def ipw_ht(Z, Y, pscores):\n",
    "\n",
    "#     weights = Z / pscores + (1 - Z) / (1 - pscores)\n",
    "\n",
    "#     output = np.sum(weights * Y) / np.sum(weights)\n",
    "#     return output\n",
    "\n",
    "\n",
    "# def ipw_hayek(Z, Y, pscores):\n",
    "\n",
    "#     t1 = Z / pscores\n",
    "#     t2 = (1 - Z) / (1 - pscores)\n",
    "\n",
    "#     return np.sum(t1 * Y) / np.sum(t1) - np.sum(t2 * Y) / np.sum(t2)\n",
    "\n",
    "# hyperparameters = {\n",
    "#     \"is_causal\": True,\n",
    "#     \"num_causes\": 8,\n",
    "#     \"prior_mlp_hidden_dim\": 8,\n",
    "#     \"num_layers\": 3,\n",
    "#     \"noise_std\": 0.1,\n",
    "#     \"y_is_effect\": True,\n",
    "#     \"pre_sample_weights\": False,\n",
    "#     \"prior_mlp_dropout_prob\": 0.5,\n",
    "#     \"pre_sample_causes\": False,\n",
    "#     \"prior_mlp_activations\": Sigmoid,\n",
    "#     \"block_wise_dropout\": True,\n",
    "#     \"init_std\": 1.0,\n",
    "#     'prior_mlp_scale_weights_sqrt': True,\n",
    "#     'sampling': 'normal',\n",
    "#     'in_clique': False,\n",
    "#     'sort_features': False,\n",
    "#     'random_feature_rotation': False,\n",
    "#     'new_mlp_per_example': True\n",
    "# } \n",
    "\n",
    "# x, y, _ = get_batch(batch_size=100, seq_len=10000, num_features=8, hyperparameters=hyperparameters,\n",
    "#           num_outputs=3, sampling=\"mixed\")\n",
    "# x = x.numpy()\n",
    "# y = y.numpy()\n",
    "# print(x.shape, y.shape)\n",
    "\n",
    "# plt.scatter(x[:,0,0], x[:,0,1], c=y[:,0,0], s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATEs = []\n",
    "# ATE_hayeks = []\n",
    "# for i in range(100):\n",
    "#     X = x[:,i,:]\n",
    "#     Y0 = y[:,i,0]\n",
    "#     Z = y[:,i,1]\n",
    "#     Z = Z > np.median(Z)\n",
    "#     Y1 = Y0 + 1#np.abs(y[:,i,2]) # 1\n",
    "#     Y = np.where(Z, Y1, Y0)\n",
    "#     ATE = np.mean(Y1) - np.mean(Y0)\n",
    "#     # print(ATE)\n",
    "#     ATEs.append(ATE)\n",
    "\n",
    "#     pscore_model = LogisticRegression()\n",
    "#     pscore_model.fit(X, Z)\n",
    "#     pscores = pscore_model.predict_proba(X)[:, 1]\n",
    "#     pscores = truncate(pscores, 0.01)\n",
    "\n",
    "#     ATE_hayek = ipw_hayek(Z, Y, pscores)\n",
    "#     ATE_hayeks.append(ATE_hayek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"is_causal\": True,\n",
    "    \"num_causes\": 1000,\n",
    "    \"prior_mlp_hidden_dim\": 1000,\n",
    "    \"num_layers\": 2,\n",
    "    \"noise_std\": 0.1,\n",
    "    \"y_is_effect\": True,\n",
    "    \"pre_sample_weights\": False,\n",
    "    \"prior_mlp_dropout_prob\": 0,\n",
    "    \"pre_sample_causes\": False,\n",
    "    \"prior_mlp_activations\": ReLU,\n",
    "    \"block_wise_dropout\": False,\n",
    "    \"init_std\": 1.0,\n",
    "    'prior_mlp_scale_weights_sqrt': True,\n",
    "    'sampling': 'normal',\n",
    "    'in_clique': False,\n",
    "    'sort_features': False,\n",
    "    'random_feature_rotation': False,\n",
    "    'new_mlp_per_example': True\n",
    "}\n",
    "\n",
    "_, x, _ = get_batch(batch_size=100, seq_len=10000, num_features=8, hyperparameters=hyperparameters,\n",
    "          num_outputs=2, sampling=\"mixed\")\n",
    "# _, u, _ = get_batch(batch_size=100, seq_len=10000, num_features=8, hyperparameters=hyperparameters,\n",
    "#           num_outputs=3, sampling=\"mixed\")\n",
    "\n",
    "# x = x.numpy()\n",
    "# u = u.numpy()\n",
    "x = x.detach()\n",
    "# u = u.detach()\n",
    "\n",
    "# print(x.shape, y.shape)\n",
    "\n",
    "# plt.scatter(x[:,0,0], x[:,0,1], c=y[:,0,0], s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape#, u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causes = [[x[:, i, np.newaxis, :]] for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of 10000 x, u pairs\n",
    "causes = [[x[:,i,np.newaxis,:], u[:,i,np.newaxis,:]] for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters['num_causes'] = x.shape[-1]\n",
    "_, y, _ = get_batch(batch_size=100, seq_len=10000, num_features=8, hyperparameters=hyperparameters,\n",
    "          num_outputs=2, sampling=\"mixed\", causes=causes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y[:,1,0].numpy(), bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    fig, axs = plt.subplots(1,3,figsize=(15, 5))\n",
    "    axs[0].scatter(x[:,i,0].numpy(), y[:,i,0].numpy(), s=2)\n",
    "    axs[1].scatter(x[:,i,1].numpy(), y[:,i,0].numpy(), s=2)\n",
    "    axs[2].scatter(x[:,i,0].numpy(), x[:,i,1].numpy(), c=y[:,i,0].numpy(), s=2)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [2,4,6,8,10,20,30,40,50,60,70,80,90,100]\n",
    "avg_skew_x1 = []\n",
    "avg_skew_x2 = []\n",
    "avg_skew_y = []\n",
    "avg_kurtosis_x1 = []\n",
    "avg_kurtosis_x2 = []\n",
    "avg_kurtosis_y = []\n",
    "r2_results = {}\n",
    "for i in grid:\n",
    "    hyperparameters = {\n",
    "        \"is_causal\": True,\n",
    "        \"num_causes\": i,\n",
    "        \"prior_mlp_hidden_dim\": i,\n",
    "        \"num_layers\": 2,\n",
    "        \"noise_std\": 0.1,\n",
    "        \"y_is_effect\": True,\n",
    "        \"pre_sample_weights\": False,\n",
    "        \"prior_mlp_dropout_prob\": 0.5,\n",
    "        \"pre_sample_causes\": False,\n",
    "        \"prior_mlp_activations\": ReLU,\n",
    "        \"block_wise_dropout\": False,\n",
    "        \"init_std\": 1.0,\n",
    "        'prior_mlp_scale_weights_sqrt': True,\n",
    "        'sampling': 'normal',\n",
    "        'in_clique': False,\n",
    "        'sort_features': False,\n",
    "        'random_feature_rotation': False,\n",
    "        'new_mlp_per_example': True\n",
    "    }\n",
    "\n",
    "    _, x, _ = get_batch(batch_size=500, seq_len=5000, num_features=8, hyperparameters=hyperparameters,\n",
    "            num_outputs=2, sampling=\"mixed\")\n",
    "    # _, u, _ = get_batch(batch_size=100, seq_len=10000, num_features=8, hyperparameters=hyperparameters,\n",
    "    #         num_outputs=3, sampling=\"mixed\")\n",
    "\n",
    "    # x = x.numpy()\n",
    "    # u = u.numpy()\n",
    "    x = x.detach()\n",
    "    # u = u.detach()\n",
    "    \n",
    "    r2_results[i] = np.zeros((x.shape[1], 10))  # Store R^2 for each degree\n",
    "\n",
    "    # make list of 10000 x, u pairs\n",
    "    causes = [[x[:,i,np.newaxis,:]] for i in range(x.shape[1])]\n",
    "\n",
    "    hyperparameters[\"num_causes\"] = x.shape[-1]\n",
    "    _, y, _ = get_batch(batch_size=500, seq_len=5000, num_features=8, hyperparameters=hyperparameters,\n",
    "            num_outputs=1, sampling=\"mixed\", causes=causes)\n",
    "    x = x.numpy()\n",
    "    skew_arr_x1 = []\n",
    "    skew_arr_x2 = []\n",
    "    skew_arr_y = []\n",
    "    kurtosis_arr_x1 = []\n",
    "    kurtosis_arr_x2 = []\n",
    "    kurtosis_arr_y = []\n",
    "    for dataset in range(x.shape[1]):\n",
    "        x1 = x[:, dataset, 0]\n",
    "        x2 = x[:, dataset, 1]\n",
    "        y0 = y[:, dataset].numpy()\n",
    "        # get skew of x1, x2, y\n",
    "        skew_x1 = np.abs(np.mean((x1 - np.mean(x1))**3) / (np.std(x1)**3))\n",
    "        skew_x2 = np.abs(np.mean((x2 - np.mean(x2))**3) / (np.std(x2)**3))\n",
    "        skew_y = np.abs(np.mean((y0 - np.mean(y0))**3) / (np.std(y0)**3))\n",
    "        skew_arr_x1.append(skew_x1)\n",
    "        skew_arr_x2.append(skew_x2)\n",
    "        skew_arr_y.append(skew_y)\n",
    "        # get kurtosis of x1, x2, y\n",
    "        kurtosis_x1 = np.mean((x1 - np.mean(x1))**4) / (np.std(x1)**4) - 3\n",
    "        kurtosis_x2 = np.mean((x2 - np.mean(x2))**4) / (np.std(x2)**4) - 3\n",
    "        kurtosis_y = np.mean((y0 - np.mean(y0))**4) / (np.std(y0)**4) - 3\n",
    "        kurtosis_arr_x1.append(kurtosis_x1)\n",
    "        kurtosis_arr_x2.append(kurtosis_x2)\n",
    "        kurtosis_arr_y.append(kurtosis_y)\n",
    "        \n",
    "        X_train = x[:, dataset, :]\n",
    "        y_train = y[:, dataset].numpy()\n",
    "        # fit polynomial regression model for different degrees\n",
    "        \n",
    "        for degree in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "            from sklearn.preprocessing import PolynomialFeatures\n",
    "            from sklearn.linear_model import LinearRegression\n",
    "            \n",
    "            poly = PolynomialFeatures(degree=degree)\n",
    "            X_poly_train = poly.fit_transform(X_train)\n",
    "            \n",
    "            model = LinearRegression()\n",
    "            model = model.fit(X_poly_train, y_train)\n",
    "            \n",
    "            # You can calculate metrics like MSE or R^2 here if needed\n",
    "            r2_results[i][dataset, degree-1] = model.score(X_poly_train, y_train)\n",
    "        \n",
    "    skew_arr_x1 = np.array(skew_arr_x1)\n",
    "    skew_arr_x2 = np.array(skew_arr_x2)\n",
    "    skew_arr_y = np.array(skew_arr_y)\n",
    "    kurtosis_arr_x1 = np.array(kurtosis_arr_x1)\n",
    "    kurtosis_arr_x2 = np.array(kurtosis_arr_x2)\n",
    "    kurtosis_arr_y = np.array(kurtosis_arr_y)\n",
    "    avg_skew_x1.append(np.median(skew_arr_x1))\n",
    "    avg_skew_x2.append(np.median(skew_arr_x2))\n",
    "    avg_skew_y.append(np.median(skew_arr_y))\n",
    "    avg_kurtosis_x1.append(np.median(kurtosis_arr_x1))\n",
    "    avg_kurtosis_x2.append(np.median(kurtosis_arr_x2))\n",
    "    avg_kurtosis_y.append(np.median(kurtosis_arr_y))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # print(\"Skewness for Width:\", i)\n",
    "    # plt.hist(skew_arr_x1, bins=20, alpha=0.5, label='X1 Skewness', histtype='step', linewidth=4)\n",
    "    # plt.hist(skew_arr_x2, bins=20, alpha=0.5, label='X2 Skewness', histtype='step', linewidth=4)\n",
    "    # plt.hist(skew_arr_y, bins=20, alpha=0.5, label='Y Skewness', histtype='step', linewidth=4)\n",
    "    # plt.legend()\n",
    "    # plt.title(f'Skewness Distribution for Width {i}')\n",
    "    # plt.show()\n",
    "    # print(\"Kurtosis for Width:\", i)\n",
    "    # plt.hist(kurtosis_arr_x1, bins=20, alpha=0.5, label='X1 Kurtosis', histtype='step', linewidth=4)\n",
    "    # plt.hist(kurtosis_arr_x2, bins=20, alpha=0.5, label='X2 Kurtosis', histtype='step', linewidth=4)\n",
    "    # plt.hist(kurtosis_arr_y, bins=20, alpha=0.5, label='Y Kurtosis', histtype='step', linewidth=4)\n",
    "    # plt.legend()\n",
    "    # plt.title(f'Kurtosis Distribution for Width {i}')\n",
    "    # plt.show()\n",
    "    \n",
    "# lineplot of average skewness vs width\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(grid, avg_skew_x1, marker='o', label='Average Skewness X1')\n",
    "plt.plot(grid, avg_skew_x2, marker='o', label='Average Skewness X2')\n",
    "plt.plot(grid, avg_skew_y, marker='o', label='Average Skewness Y')\n",
    "plt.xlabel('Width of MLP')\n",
    "plt.ylabel('Average Skewness')\n",
    "plt.title('Average Skewness vs Width of MLP')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# lineplot of average kurtosis vs width\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(grid, avg_kurtosis_x1, marker='o', label='Average Kurtosis X1')\n",
    "plt.plot(grid, avg_kurtosis_x2, marker='o', label='Average Kurtosis X2')\n",
    "plt.plot(grid, avg_kurtosis_y, marker='o', label='Average Kurtosis Y')\n",
    "plt.xlabel('Width of MLP')\n",
    "plt.ylabel('Average Kurtosis')\n",
    "plt.title('Average Kurtosis vs Width of MLP')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_results[2][1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_results[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lineplot of r2 results vs width\n",
    "r2_results_avg = {degree: [] for degree in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "for degree in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    for width in grid:\n",
    "        r2_results_avg[degree].append(np.mean(r2_results[width][:, degree-1]))\n",
    "plt.figure(figsize=(10, 5))\n",
    "for degree in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    plt.plot(grid, r2_results_avg[degree], marker='o', label=f'Average R^2 Degree {degree}')\n",
    "plt.xlabel('Width of MLP')\n",
    "plt.ylabel('Average R^2')\n",
    "plt.title('Average R^2 vs Width of MLP')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the r2 results for each width and degree with x axis degree and y axis r2\n",
    "plt.figure(figsize=(10, 5))\n",
    "for width in grid:\n",
    "    plt.plot(range(1, 11), r2_results[width].mean(axis=0), marker='o', label=f'Width {width}')\n",
    "plt.xlabel('Degree of Polynomial')\n",
    "plt.ylabel('Average R^2')\n",
    "plt.title('Average R^2 vs Degree of Polynomial for Different Widths of MLP')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regress y on x\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "r2_arr = []\n",
    "for dataset in range(100):\n",
    "    regression_model = LinearRegression()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x[:, dataset, :], y[:, dataset, 0], test_size=0.3, random_state=42)\n",
    "    regression_model.fit(X_train, y_train)\n",
    "    y_pred = regression_model.predict(X_test)\n",
    "    r2_arr.append(regression_model.score(X_test, y_test))\n",
    "r2_arr = np.array(r2_arr)\n",
    "\n",
    "# plot distribution of R^2 values\n",
    "plt.hist(r2_arr, bins=20)\n",
    "plt.xlabel('R^2 Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of R^2 Values from Linear Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rands = np.random.normal(0, 1, size=(10000, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.corrcoef(y[:,0,0], rands[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(y[:,0,0], x[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(y[:,0,0], u[:,0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y[:,0,0], x[:,0,0], s=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y[:,0,0], u[:,0,2], s=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
